{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting_Model\n",
    "This is the model which combines all the models we have trained before. This is an heuristic model which will not garantee good result. However, interesting result will be generated.\n",
    "\n",
    "The first thing to do is always import all the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import join as opj\n",
    "%matplotlib inline\n",
    "\n",
    "#Import Keras.\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"getModel()\" is the same as those ones we used before, in \"modelX\" notebooks. We use the same CNN structure so that the weight we pre-generated can be load into those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    #Building the model\n",
    "    gmodel=Sequential()\n",
    "    #Conv Layer 1\n",
    "    gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 1)))\n",
    "    gmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 2\n",
    "    gmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n",
    "    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 3\n",
    "    gmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    gmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    gmodel.add(Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    gmodel.add(Dense(512))\n",
    "    gmodel.add(Activation('relu'))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    gmodel.add(Dense(256))\n",
    "    gmodel.add(Activation('relu'))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    gmodel.add(Dense(1))\n",
    "    gmodel.add(Activation('sigmoid'))\n",
    "\n",
    "    mypotim=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    gmodel.compile(loss='binary_crossentropy',\n",
    "                  optimizer=mypotim,\n",
    "                  metrics=['accuracy'])\n",
    "    return gmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 8 models and load weight we pre-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = getModel()\n",
    "model2 = getModel()\n",
    "model3 = getModel()\n",
    "model4 = getModel()\n",
    "model5 = getModel()\n",
    "model6 = getModel()\n",
    "model7 = getModel()\n",
    "model8 = getModel()\n",
    "\n",
    "model1.load_weights(filepath=\"model_weights_b1.hdf5\")\n",
    "model2.load_weights(filepath=\"model_weights_b2.hdf5\")\n",
    "model3.load_weights(filepath=\"model_weights_b3.hdf5\")\n",
    "model4.load_weights(filepath=\"model_weights_b4.hdf5\")\n",
    "model5.load_weights(filepath=\"model_weights_b5.hdf5\")\n",
    "model6.load_weights(filepath=\"model_weights_b6.hdf5\")\n",
    "model7.load_weights(filepath=\"model_weights_b7.hdf5\")\n",
    "model8.load_weights(filepath=\"model_weights_b8.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same backgroundDimmer to get the same input data as in \"modelX\" notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backgroundDimmer(bandx, inc_angles, lmd=3, alpha=0.1):\n",
    "    lmd = lmd # affects the threshold \n",
    "    alpha = alpha\n",
    "    inc_angles = inc_angles\n",
    "    tmp = []\n",
    "    \n",
    "    for i in range(len(inc_angles)):\n",
    "        if inc_angles[i] == \"na\":\n",
    "            inc_angles[i] = 0\n",
    "            \n",
    "    inc_angle_max = max(inc_angles)\n",
    "    \n",
    "    # for those whose inc_angle is \"na\", we'll simply ignore its inc_angle effect on data, thus we set those value to max\n",
    "    for i in range(len(inc_angles)):\n",
    "        if inc_angles[i] == 0:\n",
    "            inc_angles[i] = inc_angle_max \n",
    "    \n",
    "    for x in range(bandx.shape[0]):\n",
    "        b = bandx[x].flatten()\n",
    "        bvar = np.var(b)\n",
    "        bmax = np.max(b)\n",
    "        bmin = np.min(b)\n",
    "        distance_min_max = bmax - bmin\n",
    "        threshold = bmax - lmd*bvar\n",
    "    \n",
    "        for i in range(b.shape[0]):\n",
    "                if b[i] < threshold:\n",
    "                    b[i] = b[i] - alpha * (np.cos(inc_angles[x]) - np.cos(inc_angle_max)) * \\\n",
    "                        distance_min_max/(bmax - b[i]) * bvar\n",
    "                        \n",
    "                        \n",
    "        b = b.reshape(75,75)\n",
    "        tmp.append(b)\n",
    "        \n",
    "    result = np.dstack(tmp).T\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the results with corresponding inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"../input/train.json\")\n",
    "inc_angles = train.copy()['inc_angle']\n",
    "X_band_1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "\n",
    "\n",
    "X_band_1 = X_band_1[:, :, :, np.newaxis]\n",
    "X_band_2 = X_band_2[:, :, :, np.newaxis]\n",
    "X_band_3 = (X_band_1+X_band_2)/2\n",
    "X_band_4 = X_band_1 - X_band_2\n",
    "X_band_5 = backgroundDimmer(X_band_1, inc_angles)\n",
    "X_band_6 = backgroundDimmer(X_band_2, inc_angles)\n",
    "X_band_5 = X_band_5[:, :, :, np.newaxis]\n",
    "X_band_6 = X_band_6[:, :, :, np.newaxis]\n",
    "X_band_7 = (X_band_5 + X_band_6)/2\n",
    "X_band_8 = X_band_5 - X_band_6\n",
    "target_train=train['is_iceberg']\n",
    "\n",
    "result1 = model1.predict(X_band_1)\n",
    "result2 = model2.predict(X_band_2)\n",
    "result3 = model3.predict(X_band_3)\n",
    "result4 = model4.predict(X_band_4)\n",
    "\n",
    "result5 = model5.predict(X_band_5)\n",
    "result6 = model6.predict(X_band_6)\n",
    "result7 = model7.predict(X_band_7)\n",
    "result8 = model8.predict(X_band_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"decodeResults()\" is a helper method, which will decode the result we generated, which should be possibilties, and turn them into 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decodeResults(result):\n",
    "    num_samples = result.shape[0]\n",
    "    r = np.zeros(result.shape)\n",
    "    for i in range(num_samples):\n",
    "        if result[i] >= 0.5:\n",
    "            r[i] = 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the results into a list for easy manipulation on results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results are decoded ...\n",
    "results = []\n",
    "results.append(decodeResults(result1))\n",
    "results.append(decodeResults(result2))\n",
    "results.append(decodeResults(result3))\n",
    "results.append(decodeResults(result4))\n",
    "results.append(decodeResults(result5))\n",
    "results.append(decodeResults(result6))\n",
    "results.append(decodeResults(result7))\n",
    "results.append(decodeResults(result8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this boosting model, we need the rescale weights every time we got on candidate, and get new score for every round. For further information, please check out our report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re-scale the weights\n",
    "def rescaleWeights(result, target_train, error_weights):\n",
    "    \n",
    "    n = target_train.shape[0]\n",
    "    for i in range(n):\n",
    "        if result[i] == target_train[i]:\n",
    "            error_weights[i][0] = 1\n",
    "            \n",
    "    # sum of error weights\n",
    "    sew = 0\n",
    "    # sum of correct weights\n",
    "    scw = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        if error_weights[i,0] == 1:\n",
    "            scw += error_weights[i][1]\n",
    "        else:\n",
    "            sew += error_weights[i][1]\n",
    "            \n",
    "    for i in range(n):\n",
    "        if error_weights[i][0] == 1:\n",
    "            error_weights[i][1] /= 2*scw\n",
    "        else:\n",
    "            error_weights[i][1] /= 2*sew\n",
    "            \n",
    "    return error_weights\n",
    "\n",
    "\n",
    "def getScore(result, target_train, error_weights):\n",
    "    score = 0\n",
    "    for i in range(result.shape[0]):\n",
    "        if result[i] == target_train[i]:\n",
    "            score += error_weights[i][1]\n",
    "            \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# col 0 is labeled as correct(1) or not(0), col 1 is weights\n",
    "num_samples = target_train.shape[0]\n",
    "error_weights = np.zeros((num_samples,2)).astype(float)\n",
    "for i in range(num_samples):\n",
    "    error_weights[i][1] = 1/num_samples # initially the weights are unified\n",
    "\n",
    "scores = []\n",
    "for i in range(8):\n",
    "    scores.append(getScore(results[i], target_train, error_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the first best model \n",
    "The best model is model7, which is coherent to the result in \"model7\" notebook. The output says 6, which is the index of model7's result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(scores)):\n",
    "   if scores[i] == max(scores):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the coefficient of this model is pretty high, which is the way it supposed to be. High coefficient means it has a very good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.09999925211\n"
     ]
    }
   ],
   "source": [
    "a1 = 0.5 * np.log(scores[6]/(1-scores[6]))\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the second best model \n",
    "The second best model is model8. The coefficient is not that high as the first one, yet it is still good, regarding to the fact that it should cover some error made by model7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "scores[6] = 0\n",
    "error_weights = rescaleWeights(results[6], target_train, error_weights)\n",
    "\n",
    "for i in range(8):\n",
    "    if scores[i] != 0:\n",
    "        scores[i] = getScore(results[i], target_train, error_weights)\n",
    "        \n",
    "for i in range(len(scores)):\n",
    "   if scores[i] == max(scores):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7145767615\n"
     ]
    }
   ],
   "source": [
    "a2 = 0.5 * np.log(scores[7]/(1-scores[7]))\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the third best model \n",
    "Here we can see that the weights of the third best model drops significantly. This is shows those models might not work well in a collaborative way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "scores[7] = 0\n",
    "error_weights = rescaleWeights(results[7], target_train, error_weights)\n",
    "\n",
    "for i in range(8):\n",
    "    if scores[i] != 0:\n",
    "        scores[i] = getScore(results[i], target_train, error_weights)\n",
    "        \n",
    "for i in range(len(scores)):\n",
    "   if scores[i] == max(scores):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.293265692295\n"
     ]
    }
   ],
   "source": [
    "a3 = 0.5 * np.log(scores[5]/(1-scores[5]))\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "scores[5] = 0\n",
    "error_weights = rescaleWeights(results[5], target_train, error_weights)\n",
    "\n",
    "for i in range(8):\n",
    "    if scores[i] != 0:\n",
    "        scores[i] = getScore(results[i], target_train, error_weights)\n",
    "        \n",
    "for i in range(len(scores)):\n",
    "   if scores[i] == max(scores):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.223351034135\n"
     ]
    }
   ],
   "source": [
    "a4 = 0.5 * np.log(scores[4]/(1-scores[4]))\n",
    "print(a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to the 5th model, the coefficient drops deep down, which means it hardly making any contribution at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "scores[4] = 0\n",
    "error_weights = rescaleWeights(results[4], target_train, error_weights)\n",
    "\n",
    "for i in range(8):\n",
    "    if scores[i] != 0:\n",
    "        scores[i] = getScore(results[i], target_train, error_weights)\n",
    "        \n",
    "for i in range(len(scores)):\n",
    "   if scores[i] == max(scores):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0048922639956\n"
     ]
    }
   ],
   "source": [
    "a5 = 0.5 * np.log(scores[3]/(1-scores[3]))\n",
    "print(a5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of the last model we tried to extract is negative, which means this model cannot cover the errors made by previous model at all. Even worse than fliping a coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "scores[3] = 0\n",
    "error_weights = rescaleWeights(results[3], target_train, error_weights)\n",
    "\n",
    "for i in range(8):\n",
    "    if scores[i] != 0:\n",
    "        scores[i] = getScore(results[i], target_train, error_weights)\n",
    "        \n",
    "for i in range(len(scores)):\n",
    "   if scores[i] == max(scores):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.430128170632\n"
     ]
    }
   ],
   "source": [
    "a6 = 0.5 * np.log(scores[0]/(1-scores[0]))\n",
    "print(a6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result of Boosting Model\n",
    "The final result is worse than only using the best model, which has the accuracy of nearly 90%. Now we can conclude that the boosting model failed to put our classifiers together in a positive way. However, this is a very interesting approach that we can find the error overlapping between classifiers. In some situation, this is very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7400249376558603\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    for j in range(len(results[0])):\n",
    "        if results[i][j] == 0:\n",
    "            results[i][j] == -1\n",
    "\n",
    "\n",
    "\n",
    "final_result = np.zeros(target_train.shape)\n",
    "for i in range(final_result.shape[0]):\n",
    "    if a1*results[6][i] + a2*results[7][i] + a3*results[5][i] + a4*results[4][i] > 0:\n",
    "        final_result[i] = 1\n",
    "\n",
    "s = 0\n",
    "for i in range(final_result.shape[0]):\n",
    "    if final_result[i] == target_train[i]:\n",
    "        s += 1\n",
    "        \n",
    "print(\"\"s/target_train.shape[0]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
